{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Services Personalizer \n",
    "https://github.com/Azure-Samples/cognitive-services-personalizer-samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial simulates a Personalizer loop _system_ which suggests which type of coffee a customer should order. The users and their preferences are stored in a [user dataset](users.json). Information about the coffee is also available in a [coffee dataset](coffee.json)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the user information is:\n",
    "\n",
    "|Customers|Times of Day|Types of weather|\n",
    "|--|--|--|\n",
    "|Alice<br>Bob<br>Cathy<br>Dave|Morning<br>Afternoon<br>Evening|Sunny<br>Rainy<br>Snowy| \n",
    "\n",
    "To help Personalizer make the correct coffee selection for each person, the _system_ also knows details about the coffee.\n",
    "\n",
    "|Types of temperature|Places of origin|Types of roast|Organic|\n",
    "|--|--|--|--|\n",
    "|Hot<br>Cold|Kenya<br>Brazil<br>Ethiopia|Dark<br>Light|Organic<br>Not organic|\n",
    "\n",
    "The **purpose** of the Personalizer loop is to find the best match between the users and the coffee as much of the time as possible. \n",
    "\n",
    "The code for this tutorial is available in the [Personalizer Samples GitHub repository](https://github.com/Azure-Samples/cognitive-services-personalizer-samples/blob/master/samples/azurenotebook/Personalization.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the simulation works\n",
    "\n",
    "At the beginning of the running system, the suggestions from Personalizer are only successful between 20% to 30% (indicated by the reward score of 1). After some requests, the system improves.\n",
    "\n",
    "After the initial 10,000 requests, run an offline evaluation. This allows Personalizer to review the data and suggest a better learning policy. Apply the new learning policy and run the notebook again with 2,000 requests. The loop will perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank and reward calls\n",
    "\n",
    "For each of the few thousand calls to the Personalizer service, the Azure notebook sends the **Rank** request to the REST API:\n",
    "\n",
    "* A unique ID for the Rank/Request event\n",
    "* Context - A random choice of the user, weather, and time of day - simulating a user on a website or mobile device\n",
    "* Features - _All_ the coffee data - from which Personalizer makes a suggestion\n",
    "\n",
    "The system receives the rank of the coffee choices, then compares that prediction with the user's known choice for the same time of day and weather. If the known choice is the same as the predicted choice, the **Reward** of 1 is sent back to Personalizer. Otherwise the reward is 0. \n",
    "\n",
    "> [!Note]\n",
    "> This is a simulation so the algorithm for the reward is simple. In a real-world scenario, the algorithm should use business logic, possibly with weights for various aspects of the customer's experience, to determine the reward score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* An [Azure Personalizer resource](https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesPersonalizer). \n",
    "    * If you have already used the Personalizer resource, make sure to [clear the data](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/how-to-settings#clear-data-for-your-learning-loop) in the Azure portal for the resource. \n",
    "* Upload all the files for [this sample](https://github.com/Azure-Samples/cognitive-services-personalizer-samples/tree/master/samples/azurenotebook) into an Azure Notebook project.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "File descriptions:\n",
    "\n",
    "* [Personalizer.ipynb](https://github.com/Azure-Samples/cognitive-services-personalizer-samples/blob/master/samples/azurenotebook/Personalization.ipynb)  is the Jupyter notebook for this tutorial.\n",
    "* [User dataset](https://github.com/Azure-Samples/cognitive-services-personalizer-samples/blob/master/samples/azurenotebook/users.json) is stored in a JSON object.\n",
    "* [Coffee dataset](https://github.com/Azure-Samples/cognitive-services-personalizer-samples/blob/master/samples/azurenotebook/coffee.json) is stored in a JSON object. \n",
    "* [Example Request JSON](https://github.com/Azure-Samples/cognitive-services-personalizer-samples/blob/master/samples/azurenotebook/example-rankrequest.json) is the expected format for a POST request to the Rank API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Personalizer resource\n",
    "\n",
    "In the Azure portal, configure your [Personalizer resource](https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesPersonalizer) with the **update model frequency** set to 15 seconds and a **reward wait time** of 15 seconds. These settings are found in the in the Configurations tab, under the RESOURCE MANAGEMENT section. \n",
    "\n",
    "|Setting|Value|\n",
    "|--|--|\n",
    "|update model frequency|15 seconds|\n",
    "|reward wait time|15 seconds|\n",
    "\n",
    "These values have a very short duration in order to show changes in this tutorial. These values shouldn't be used in a production scenario without validating they achieve your goal with your Personalizer loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Azure Notebook\n",
    "\n",
    "Change the Kernel to `Python 3.6`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Notebook cells\n",
    "\n",
    "Run each executable cell and wait for it to return. \n",
    "\n",
    "You know it is done when the brackets next to the cell display a number instead of a `*`. Do not continue if you get an error. \n",
    "\n",
    "The following sections explain what each cell does programmatically and what to expect for the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include the python modules\n",
    "\n",
    "Include the required python modules. The cell has no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import requests\n",
    "import time\n",
    "import uuid\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Personalizer resource key and name\n",
    "\n",
    "From the Azure portal, find your key and endpoint on the **Quickstart** page of your Personalizer resource. Change the value of `<your-resource-name>` to your Personalizer resource's name. Change the value of `<your-resource-key>` to your Personalizer key. \n",
    "\n",
    "The cell has no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'personalization_base_url' and 'resource_key' with your valid endpoint values.\n",
    "personalization_base_url = \"https://<your-resource-name>.cognitiveservices.azure.com/\"\n",
    "resource_key = \"<your-resource-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print current data and time\n",
    "Use this function to note the start and end times of the iterative function, `iterations`.\n",
    "\n",
    "These cells have no output. The function does output the current date and time when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out current datetime\n",
    "def currentDateTime():\n",
    "    currentDT = datetime.datetime.now()\n",
    "    print (str(currentDT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the last model update date and time\n",
    "\n",
    "When the function, `get_last_updated`, is called, the function prints out the last modified date and time that the model was updated. \n",
    "\n",
    "These cells have no output. The function does output the last model training date when called.\n",
    "\n",
    "The function uses a GET REST API to [get model properties](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/GetModelProperties). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ititialize variable for model's last modified date\n",
    "modelLastModified = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_updated(currentModifiedDate):\n",
    "    \n",
    "    print('-----checking model')\n",
    "    \n",
    "    # get model properties\n",
    "    response = requests.get(personalization_model_properties_url, headers = headers, params = None)\n",
    "    \n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    \n",
    "    # get lastModifiedTime\n",
    "    lastModifiedTime = json.dumps(response.json()[\"lastModifiedTime\"])\n",
    "    \n",
    "    if (currentModifiedDate != lastModifiedTime):\n",
    "        currentModifiedDate = lastModifiedTime\n",
    "        print(f'-----model updated: {lastModifiedTime}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get policy and service configruation\n",
    "\n",
    "Validate the state of the service with these two REST calls.\n",
    "\n",
    "These cells have no output. The function does output the service settings when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_service_settings():\n",
    "    \n",
    "    print('-----checking service settings')\n",
    "    \n",
    "    # get learning policy\n",
    "    response = requests.get(personalization_model_policy_url, headers = headers, params = None)\n",
    "    \n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    \n",
    "    # get service settings\n",
    "    response = requests.get(personalization_service_configuration_url, headers = headers, params = None)\n",
    "    \n",
    "    print(response)\n",
    "    print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct URLs for REST calls and read JSON data files\n",
    "\n",
    "This cell \n",
    "\n",
    "* builds the URLs used in REST calls \n",
    "* sets the security header using your Personalizer resource key \n",
    "* sets the random seed for the Rank event ID\n",
    "* reads in the JSON data files\n",
    "* calls `get_last_updated` method - learning policy has been removed in example output\n",
    "* calls `get_service_settings` method\n",
    "\n",
    "The cell has output from the call to `get_last_updated` and `get_service_settings` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build URLs\n",
    "personalization_rank_url = personalization_base_url + \"personalizer/v1.0/rank\"\n",
    "personalization_reward_url = personalization_base_url + \"personalizer/v1.0/events/\" #add \"{eventId}/reward\"\n",
    "personalization_model_properties_url = personalization_base_url + \"personalizer/v1.0/model/properties\"\n",
    "personalization_model_policy_url = personalization_base_url + \"personalizer/v1.0/configurations/policy\"\n",
    "personalization_service_configuration_url = personalization_base_url + \"personalizer/v1.0/configurations/service\"\n",
    "headers = {'Ocp-Apim-Subscription-Key' : resource_key, 'Content-Type': 'application/json'}\n",
    "\n",
    "# context\n",
    "users = \"users.json\"\n",
    "\n",
    "# action features\n",
    "coffee = \"coffee.json\"\n",
    "\n",
    "# empty JSON for Rank request\n",
    "requestpath = \"example-rankrequest.json\"\n",
    "\n",
    "# initialize random\n",
    "random.seed(time.time())\n",
    "\n",
    "userpref = None \n",
    "rankactionsjsonobj = None \n",
    "actionfeaturesobj = None\n",
    "\n",
    "with open(users) as handle:\n",
    "    userpref = json.loads(handle.read())\n",
    "\n",
    "with open(coffee) as handle:\n",
    "    actionfeaturesobj = json.loads(handle.read())\n",
    "    \n",
    "with open(requestpath) as handle:\n",
    "    rankactionsjsonobj = json.loads(handle.read())  \n",
    "    \n",
    "get_last_updated(modelLastModified)\n",
    "get_service_settings()\n",
    "\n",
    "print(f'User count {len(userpref)}')\n",
    "print(f'Coffee count {len(actionfeaturesobj)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the output's `rewardWaitTime` and `modelExportFrequency` are both set to 15 seconds. \n",
    "    \n",
    "```console\n",
    "-----checking model\n",
    "<Response [200]>\n",
    "{'creationTime': '0001-01-01T00:00:00+00:00', 'lastModifiedTime': '0001-01-01T00:00:00+00:00'}\n",
    "-----model updated: \"0001-01-01T00:00:00+00:00\"\n",
    "-----checking service settings\n",
    "<Response [200]>\n",
    "{...learning policy...}\n",
    "<Response [200]>\n",
    "{'rewardWaitTime': '00:00:15', 'defaultReward': 0.0, 'rewardAggregation': 'earliest', 'explorationPercentage': 0.2, 'modelExportFrequency': '00:00:15', 'logRetentionDays': -1}\n",
    "User count 4\n",
    "Coffee count 4\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting the first REST call\n",
    "\n",
    "This previous cell is the first cell that calls out to Personalizer. Make sure the REST status code in the output is `<Response [200]>`. If you get an error, such as 404, but you are sure your resource key and name are correct, reload the notebook.\n",
    "\n",
    "Make sure the count of coffee and users is both 4. If you get an error, check that you uploaded all 3 JSON files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up metric chart in Azure portal\n",
    "\n",
    "Later in this tutorial, the long running process of 10,000 requests is visible from the browser with an updating text box. It may be easier to see in a chart or as a total sum, when the long running process ends. To view this information, use the metrics provided with the resource. You can create the chart now that you have completed a request to the service, then refresh the chart periodically while the long running process is going.\n",
    "\n",
    "1. In the Azure portal, select your Personalizer resource.\n",
    "1. In the resource navigation, select **Metrics** underneath Monitoring. \n",
    "1. In the chart, select **Add metric**.\n",
    "1. The resource and metric namespace are already set. You only need to select the metric of **successful calls** and the aggregation of **sum**.\n",
    "1. Change the time filter to the last 4 hours.\n",
    "\n",
    "    You should see three successful calls in the chart. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a unique event ID\n",
    "\n",
    "This function generates a unique ID for each rank call. The ID is used to identify the rank and reward call information. This value could come from a business process such as a web view ID or transaction ID.\n",
    "\n",
    "The cell has no output. The function does output the unique ID when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_event_id(rankjsonobj):\n",
    "    eventid = uuid.uuid4().hex\n",
    "    rankjsonobj[\"eventId\"] = eventid\n",
    "    return eventid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random user, weather, and time of day\n",
    "\n",
    "This function selects a unique user, weather, and time of day, then adds those items to the JSON object to send to the Rank request.\n",
    "\n",
    "The cell has no output. When the function is called it returns the random user's name, random weather, and random time of day.\n",
    "\n",
    "The list of 4 users and their preferences - only some preferences are shown for brevity: \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Alice\": {\n",
    "    \"Sunny\": {\n",
    "      \"Morning\": \"Cold brew\",\n",
    "      \"Afternoon\": \"Iced mocha\",\n",
    "      \"Evening\": \"Cold brew\"\n",
    "    }...\n",
    "  },\n",
    "  \"Bob\": {\n",
    "    \"Sunny\": {\n",
    "      \"Morning\": \"Cappucino\",\n",
    "      \"Afternoon\": \"Iced mocha\",\n",
    "      \"Evening\": \"Cold brew\"\n",
    "    }...\n",
    "  },\n",
    "  \"Cathy\": {\n",
    "    \"Sunny\": {\n",
    "      \"Morning\": \"Latte\",\n",
    "      \"Afternoon\": \"Cold brew\",\n",
    "      \"Evening\": \"Cappucino\"\n",
    "    }...\n",
    "  },\n",
    "  \"Dave\": {\n",
    "    \"Sunny\": {\n",
    "      \"Morning\": \"Iced mocha\",\n",
    "      \"Afternoon\": \"Iced mocha\",\n",
    "      \"Evening\": \"Iced mocha\"\n",
    "    }...\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_user_and_contextfeatures(namesoption, weatheropt, timeofdayopt, rankjsonobj):   \n",
    "    name = namesoption[random.randint(0,3)]\n",
    "    weather = weatheropt[random.randint(0,2)]\n",
    "    timeofday = timeofdayopt[random.randint(0,2)]\n",
    "    rankjsonobj['contextFeatures'] = [{'timeofday': timeofday, 'weather': weather, 'name': name}]\n",
    "    return [name, weather, timeofday]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Add all coffee data\n",
    "\n",
    "This function adds the entire list of coffee to the JSON object to send to the Rank request. \n",
    "\n",
    "The cell has no output. The function does change the `rankjsonobj` when called.\n",
    "\n",
    "\n",
    "The example of a single coffee's features is: \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": \"Cappucino\",\n",
    "    \"features\": [\n",
    "    {\n",
    "        \"type\": \"hot\",\n",
    "        \"origin\": \"kenya\",\n",
    "        \"organic\": \"yes\",\n",
    "        \"roast\": \"dark\"\n",
    "        \n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_action_features(rankjsonobj):\n",
    "    rankjsonobj[\"actions\"] = actionfeaturesobj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Rank API's prediction with known user preference\n",
    "\n",
    "This function is called after the Rank API is called, for each iteration.\n",
    "\n",
    "This function compares the user's preference for coffee, based on weather and time of day, with the Personalizer's suggestion for the user for those filters. If the suggestion matches, a score of 1 is returned, otherwise the score is 0. The cell has no output. The function does output the score when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_from_simulated_data(name, weather, timeofday, prediction):\n",
    "    if(userpref[name][weather][timeofday] == str(prediction)):\n",
    "        return 1 \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through calls to Rank and Reward\n",
    "\n",
    "The next cell is the _main_ work of the Notebook, getting a random user, getting the coffee list, sending both to the Rank API. Comparing the prediction with the user's known preferences, then sending the reward back to the Personalizer service. \n",
    "\n",
    "The loop runs for `num_requests` times. Personalizer needs a few thousand calls to Rank and Reward to create a model. \n",
    "\n",
    "An example of the JSON sent to the Rank API follows. The list of coffee is not complete, for brevity. You can see the entire JSON for coffee in `coffee.json`.\n",
    "\n",
    "\n",
    "```json\n",
    "{ \n",
    "   'contextFeatures':[ \n",
    "      { \n",
    "         'timeofday':'Evening',\n",
    "         'weather':'Snowy',\n",
    "         'name':'Alice'\n",
    "      }\n",
    "   ],\n",
    "   'actions':[ \n",
    "      { \n",
    "         'id':'Cappucino',\n",
    "         'features':[ \n",
    "            { \n",
    "               'type':'hot',\n",
    "               'origin':'kenya',\n",
    "               'organic':'yes',\n",
    "               'roast':'dark'\n",
    "            }\n",
    "         ]\n",
    "      }\n",
    "        ...rest of coffee list\n",
    "   ],\n",
    "   'excludedActions':[ \n",
    "\n",
    "   ],\n",
    "   'eventId':'b5c4ef3e8c434f358382b04be8963f62',\n",
    "   'deferActivation':False\n",
    "}\n",
    "```\n",
    "\n",
    "JSON sent to the Rank API:\n",
    "\n",
    "```console\n",
    "{'contextFeatures': [{'timeofday': 'Morning', 'weather': 'Sunny', 'name': 'Bob'}], 'actions': [{'id': 'Cappucino', 'features': [{'type': 'hot', 'origin': 'kenya', 'organic': 'yes', 'roast': 'dark'}]}, {'id': 'Cold brew', 'features': [{'type': 'cold', 'origin': 'brazil', 'organic': 'yes', 'roast': 'light'}]}, {'id': 'Iced mocha', 'features': [{'type': 'cold', 'origin': 'ethiopia', 'organic': 'no', 'roast': 'light'}]}, {'id': 'Latte', 'features': [{'type': 'hot', 'origin': 'brazil', 'organic': 'no', 'roast': 'dark'}]}], 'excludedActions': [], 'eventId': '5001bcfe3bb542a1a238e6d18d57f2d2', 'deferActivation': False}\n",
    "```\n",
    "\n",
    "JSON response from the Rank API:\n",
    "\n",
    "```\n",
    "{\n",
    "    'ranking': [\n",
    "        {'id': 'Latte', 'probability': 0.85 },\n",
    "        {'id': 'Iced mocha', 'probability': 0.05 },\n",
    "        {'id': 'Cappucino', 'probability': 0.05 },\n",
    "        {'id': 'Cold brew', 'probability': 0.05 }\n",
    "    ], \n",
    "    'eventId': '5001bcfe3bb542a1a238e6d18d57f2d2', \n",
    "    'rewardActionId': 'Latte'\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, each loop shows the random selection of user, weather, time of day, and determined reward. The reward of 1 indicates the Personalizer resource selected the correct coffee type for the given user, weather, and time of day.\n",
    "\n",
    "```console\n",
    "1 Alice Rainy Morning Latte 1\n",
    "```\n",
    "\n",
    "The function uses:\n",
    "\n",
    "* Rank: a POST REST API to [get rank](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/Rank). \n",
    "* Reward: a POST REST API to [report reward](https://westus2.dev.cognitive.microsoft.com/docs/services/personalizer-api/operations/Reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterations(n, modelCheck, jsonFormat):\n",
    "\n",
    "    i = 1\n",
    "    \n",
    "    # default reward value - assumes failed prediction\n",
    "    reward = 0\n",
    "\n",
    "    # Print out dateTime\n",
    "    #currentDateTime()\n",
    "\n",
    "    # collect results to aggregate in graph\n",
    "    total = 0\n",
    "    rewards = []\n",
    "    count = []\n",
    "\n",
    "    # default list of user, weather, time of day\n",
    "    namesopt = ['Alice', 'Bob', 'Cathy', 'Dave']\n",
    "    weatheropt = ['Sunny', 'Rainy', 'Snowy']\n",
    "    timeofdayopt = ['Morning', 'Afternoon', 'Evening']\n",
    "    \n",
    "    \n",
    "    while(i <= n):\n",
    "\n",
    "        # create unique id to associate with an event\n",
    "        eventid = add_event_id(jsonFormat)\n",
    "\n",
    "        # generate a random sample\n",
    "        [name, weather, timeofday] = add_random_user_and_contextfeatures(namesopt, weatheropt, timeofdayopt, jsonFormat)\n",
    "\n",
    "        # add action features to rank\n",
    "        add_action_features(jsonFormat) \n",
    "\n",
    "        # show JSON to send to Rank\n",
    "        #print('To: ', jsonFormat)    \n",
    "\n",
    "        # choose an action - get prediction from Personalizer\n",
    "        response = requests.post(personalization_rank_url, headers = headers, params = None, json = jsonFormat)\n",
    "\n",
    "        # show Rank prediction \n",
    "        #print ('From: ',response.json())    \n",
    "\n",
    "        # compare personalization service recommendation with the simulated data to generate a reward value\n",
    "        prediction = json.dumps(response.json()[\"rewardActionId\"]).replace('\"','')\n",
    "        reward = get_reward_from_simulated_data(name, weather, timeofday, prediction)\n",
    "\n",
    "        # show result for iteration\n",
    "        print(f'   {i} {currentDateTime()} {name} {weather} {timeofday} {prediction} {reward} ')\n",
    "\n",
    "        # send the reward to the service \n",
    "        response = requests.post(personalization_reward_url + eventid + \"/reward\", headers = headers, params= None, json = { \"value\" : reward })\n",
    "\n",
    "        # for every N rank requests, compute total correct  \n",
    "        total = total + reward\n",
    "\n",
    "        # every N iteration, get last updated model date and time\n",
    "        if(i % modelCheck == 0):\n",
    "\n",
    "            print(\"**** 10% of loop found\")\n",
    "            get_last_updated(modelLastModified) \n",
    "\n",
    "        # aggregate so chart is easier to read\n",
    "        if(i % 100 == 0):\n",
    "            print(\"**** aggregating rewards\")\n",
    "            rewards.append(total)\n",
    "            count.append(i)\n",
    "            total = 0\n",
    "\n",
    "        i = i + 1\n",
    "        \n",
    "    # Print out dateTime\n",
    "    #currentDateTime()\n",
    "    \n",
    "    return [count, rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for 10,000 iterations\n",
    "\n",
    "Run the Personalizer loop for 10,000 iterations. This is a long running event. Do not close the browser running the notebook. Refresh the metrics chart in the Azure portal periodically to see the total calls to the service. When you have around 20,000 calls, a rank and reward call for each iteration of the loop, the iterations are done. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max iterations\n",
    "num_requests = 10000\n",
    "\n",
    "# check last mod date N% of time - currently 10%\n",
    "lastModCheck = int(num_requests * .10)\n",
    "\n",
    "jsonTemplate = rankactionsjsonobj\n",
    "\n",
    "# main iterations\n",
    "[count, rewards] = iterations(num_requests, lastModCheck, jsonTemplate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Chart results to see improvement with Personalizer\n",
    "\n",
    "Create a chart from the `count` and `rewards`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChart(x, y):\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Batch of rank events\")\n",
    "    plt.ylabel(\"Correct recommendations per batch\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run chart for 10,000 rank requests\n",
    "\n",
    "Run the `createChart` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "createChart(count, rewards) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the chart\n",
    "\n",
    "This chart shows the success of the model for the current default learning policy. \n",
    "\n",
    "The ideal target that by the end of the test, the loop is averaging a success rate that is close to one hundred percent minus the exploration. The default setting of exploration is 20%. \n",
    "\n",
    "`100-20=80`\n",
    "\n",
    "This exploration setting is found in the Azure portal, for the Personalizer resource, in the Configurations tab, under the RESOURCE MANAGEMENT section \n",
    "\n",
    "In order to find a better learning policy, based on your data to the Rank API, run an [offline evaluation](how-to-offline-evaluation.md) in the portal for your Personalizer loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an offline evaluation\n",
    "\n",
    "1. In the Azure portal, open the Personalizer resource's **Evaluations** page.\n",
    "1. Select **Create Evaluation**.\n",
    "1. Enter the required data of evaluation name, and date range for the loop evaluation. The date range should include only the days you are focusing on for your evaluation. \n",
    "\n",
    "    The purpose of running this offline evaluation is to determine if there is a better learning policy for the features and actions used in this loop. To find that better learning policy, make sure **Optimization policy** is turned on.\n",
    "\n",
    "1. Select **OK** to begin the evaluation. \n",
    "1. This **Evaluations** page lists the new evaluation and its current status. Depending on how much data you have, this evaluation can take some time. You can come back to this page after a few minutes to see the results. \n",
    "1. When the evaluation is completed, select the evaluation then select **Comparison of different learning policies**. This shows the available learning policies and how they would behave with the data. \n",
    "1. Select the top-most learning policy in the table and select **Apply**. This applies the _best_ learning policy to your model and retrains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change update model frequency to 5 minutes\n",
    "\n",
    "1. In the Azure portal, still on the Personalizer resource, selec the Configurations tab, under the RESOURCE MANAGEMENT section. \n",
    "1. Change the **model update frequency** and **reward wait time** to 5 minutes and select **Save**.\n",
    "\n",
    "Learn more about the [reward wait time](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/concept-rewards#reward-wait-time) and [model update frequency](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/how-to-settings#model-update-frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify new learning policy and times\n",
    "get_service_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the output's `rewardWaitTime` and `modelExportFrequency` are both set to 5 minutes. \n",
    "    \n",
    "```console\n",
    "-----checking model\n",
    "<Response [200]>\n",
    "{'creationTime': '0001-01-01T00:00:00+00:00', 'lastModifiedTime': '0001-01-01T00:00:00+00:00'}\n",
    "-----model updated: \"0001-01-01T00:00:00+00:00\"\n",
    "-----checking service settings\n",
    "<Response [200]>\n",
    "{...learning policy...}\n",
    "<Response [200]>\n",
    "{'rewardWaitTime': '00:05:00', 'defaultReward': 0.0, 'rewardAggregation': 'earliest', 'explorationPercentage': 0.2, 'modelExportFrequency': '00:05:00', 'logRetentionDays': -1}\n",
    "User count 4\n",
    "Coffee count 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate new learning policy by running experiment for 2,000 iterations\n",
    "\n",
    "Return to the Azure notebook, and continue by running the same loop but for only 2,000 iterations. Refresh the metrics chart in the Azure portal periodically to see the total calls to the service. When you have around 4,000 calls, a rank and reward call for each iteration of the loop, the iterations are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max iterations\n",
    "num_requests = 2000\n",
    "\n",
    "# check last mod date N% of time - currently 10%\n",
    "lastModCheck2 = int(num_requests * .10)\n",
    "\n",
    "jsonTemplate2 = rankactionsjsonobj\n",
    "\n",
    "# main iterations\n",
    "[count2, rewards2] = iterations(num_requests, lastModCheck2, jsonTemplate2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run chart for 2,000 rank requests\n",
    "\n",
    "Run the `createChart` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "createChart(count2,rewards2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Review the second chart\n",
    "\n",
    "The second chart should show a visible increase in Rank predictions aligning with user preferences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "If you do not intend to continue the tutorial series, clean up the following resources:\n",
    "\n",
    "* Delete your Azure Notebook project. \n",
    "* Delete your Personalizer resource. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
